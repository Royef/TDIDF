{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa10f5df-03cb-410a-80dd-5bef65f371cb",
   "metadata": {},
   "source": [
    "# TDIDF\n",
    "\n",
    "In this assignment, you will enhance a suboptimal information retrieval system. Your task is to develop an inverted index to facilitate the rapid retrieval of documents matching specific queries. Further improve your system by implementing term-frequency inverse-document-frequency (TF-IDF) weighting and using cosine similarity for comparing queries with your dataset. The performance of your information retrieval system will be assessed based on the accuracy of the document retrieval, as well as the precision of the computed TF-IDF values and cosine similarities.\n",
    "\n",
    "\n",
    "## Data Description\n",
    "For this assignment, you will use your Information Retrieval (IR) system to locate relevant documents within a collection of sixty books sourced from Project Gutenberg. The training data can be found in the `data/` directory, specifically under the `ProjectGutenberg/` subdirectory. Here, you will find another directory named `raw/`, which contains the raw text files for the sixty short stories. Additionally, the `data/` directory includes files `dev_queries.txt` and `dev_solutions.txt`, which contain a set of development queries and their corresponding expected answers. These resources are provided to aid you as you begin to implement your IR system.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Enhance the existing IR system by developing the following functionalities:\n",
    "\n",
    "1. Inverted Positional Index: Create an inverted index, which is a mapping of words to the documents they appear in, along with the positions within those documents where the words occur.\n",
    "\n",
    "1. Boolean Retrieval: Implement a Boolean retrieval system that returns a list of documents containing all the words in a given query. For this assignment, your system should only need to support conjunctions.\n",
    "\n",
    "1. TF-IDF: Calculate and store the term-frequency inverse-document-frequency (TF-IDF) values for every word-document pairing.\n",
    "\n",
    "1. Cosine Similarity: To enhance the accuracy and relevance of document retrieval, you will implement cosine similarity. Here's a breakdown of how to apply cosine similarity with specific weighting schemes for both queries and documents:\n",
    "\n",
    "\n",
    "#### Cosine Similarity\n",
    "\n",
    "When computing the query term weight you will use $tf_{t,q} = 1+log_{10}(\\text{count}(t,q))$ with no IDF term and no normalization.\n",
    "When computing the document vector you will calculate the tf, idf and normalize it (divide by the square root of the sum of squares of the tf-idf weights) as seen in class. To calculate the cosine scores you will perform a dot product between the document vector and the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b46163-1ada-4c48-9460-e856bcb45da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import collections\n",
    "import numpy as np\n",
    "from pprint import pprint  \n",
    "from stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa30f3-ed0d-44ab-af49-e9d187ce0745",
   "metadata": {},
   "source": [
    "In this exercise, you will only work on the `IRSystem` Class.\n",
    "\n",
    "In the next cells, you will extend the functionality of this class. For better readability and ease of use, you will add additional methods to the same class by inheriting from itself. Use this example for reference:\n",
    "\n",
    "```\n",
    "class Foo:\n",
    "    def a(self):\n",
    "        print(\"a\")\n",
    "\n",
    "class Foo(Foo):\n",
    "    def b(self):\n",
    "        print(\"b\")\n",
    "\n",
    "foo = Foo()\n",
    "foo.a()\n",
    "foo.b()\n",
    "```\n",
    "The following will be printed without errors: \n",
    "```\n",
    "a\n",
    "b\n",
    "```\n",
    "The first part of the class will load and stem the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b022d-663c-40b2-95ab-2780d82d756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile(r'(.*) \\d+\\.txt')\n",
    "        \n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are no 60 documents in ../data/ProjectGutenberg/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/ProjectGutenberg/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79ce35-7cb5-4517-88df-3102c9a00107",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index. The documents will have already been read at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: \n",
    "* titles (a list of strings)\n",
    "* docs (a list of lists of strings)\n",
    "* vocab (a list of strings).\n",
    "\n",
    "Since the majority of the work in this assignment will use document ID, we recommend doing the mapping using the document IDs (i.e. the index of the document within self.docs). You can then retrieve the text and title of any given document by indexing into self.docs and self.titles respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbff866-0d21-4a9b-a709-5a8dad4bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Build an inverted positional index of the documents.\n",
    "        \"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        inv_index = {}\n",
    "\n",
    "        #iterate over each document and its index\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            \n",
    "            #iterate over each word in the document and its index\n",
    "            for pos, word in enumerate(doc):\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {}\n",
    "                if doc_id not in inv_index[word]:\n",
    "                    inv_index[word][doc_id] = []\n",
    "                inv_index[word][doc_id].append(pos)\n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "    def print_index(self):\n",
    "        \"\"\"\n",
    "        A helper function to print the inverted index.\n",
    "        This function is useful for debugging and visualizing the index.\n",
    "        \"\"\"\n",
    "        pprint(self.inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50335a2d-051d-4dbe-9f60-41890d3eeb86",
   "metadata": {},
   "source": [
    "Next we will implement `get_word_positions`. This method returns a list of integers that identifies the positions in the document doc (represented as document ID) in which the word is found. This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully.\n",
    "\n",
    "Be careful when accessing your inverted index! Trying to index into a dictionary using a key that is not present will cause an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32de8d52-110e-4a59-9f03-0a8f35e9eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_word_positions(self, word, doc):\n",
    "        \"\"\"\n",
    "        Given a word and a document, use the inverted index to return\n",
    "        the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        if word in self.inv_index and doc in self.inv_index[word]:\n",
    "            return self.inv_index[word][doc]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30cf34-eaa2-4626-91ca-a58d54b3002f",
   "metadata": {},
   "source": [
    "We will add another method, `get_posting`, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95c5b34-d7f7-4d0f-9f5e-22a58c4ca8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        if word in self.inv_index:\n",
    "            return list(self.inv_index[word].keys())\n",
    "        return []\n",
    "\n",
    "    \n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_posting(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca144b62-d3f7-4992-88ec-73aca8594969",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in the query occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4699ba36-3cef-4069-94b0-a02498562b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "         #check if the query is empty, if so - returns empty list\n",
    "        if not query:\n",
    "            return []\n",
    "\n",
    "        #im using 'set' because its return unique values\n",
    "        #'get' is used if the term in the query is not found it will return empty list\n",
    "        docs = set(self.inv_index.get(query[0], []))\n",
    "\n",
    "        #from the second term for every other term in the query i check if the term is in the inverted index.\n",
    "        for term in query[1:]:\n",
    "            if term in self.inv_index:\n",
    "                docs &= set(self.inv_index[term])\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        return sorted(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2c25b-5e06-44b9-a7bc-041ed280b9d6",
   "metadata": {},
   "source": [
    "Now we will compute and score the tf-idf values. `compute_tfidf` stores the tf-idf values for words and documents. For this, you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement get_tfidf to return the tf-idf weight for a particular word and document ID. Make sure you correctly handle the case where the word isn't present in your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe8f294-9dc3-470d-af0a-19c74892c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def compute_tfidf(self):\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        N = len(self.docs)  #total number of documents\n",
    "        self.tfidf = {}  #create tf-idf dictionary\n",
    "\n",
    "        for word in self.vocab:\n",
    "            df_t = len(self.get_posting(word))  #df_t - number of documents that t appears in\n",
    "            idf_t = math.log10(N / df_t) if df_t > 0 else 0  #avoid division by zero\n",
    "\n",
    "            for doc_id in self.get_posting(word):\n",
    "                tf_t_d = len(self.get_word_positions(word, doc_id))  #tf_t_d - number of t appears in d\n",
    "                tf_idf_t_d = (1 + math.log10(tf_t_d)) * idf_t if tf_t_d > 0 else 0  #avoid log(0)\n",
    "                \n",
    "                if doc_id not in self.tfidf:\n",
    "                    self.tfidf[doc_id] = {}  \n",
    "                \n",
    "                self.tfidf[doc_id][word] = tf_idf_t_d  #store the tf-idf value for the word in the document\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        tfidf = 0.0\n",
    "        if word in self.vocab and document in self.tfidf and word in self.tfidf[document]:\n",
    "            tfidf = self.tfidf[document][word]\n",
    "        return tfidf\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        This function gets the TF-IDF of an *unstemmed* word in a document.\n",
    "        Stems the word and then calls get_tfidf. You should *not* need to\n",
    "        change this interface, but it is necessary for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_tfidf(word, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcabca3-c52a-4a83-8ea6-8ac164be8694",
   "metadata": {},
   "source": [
    "Lastly, we will implement `rank_retrieve`. This function returns sorted list of the top ranked documents for a given query.\n",
    "\n",
    "Remember to use ltc.lnn weighting, that is, ltc weighting for the document and lnn weighting for the query! This means that the query vector weights will be $tf_{t,q} = 1+log_{10}(\\text{count}(t,q))$ with no IDF term or normalization (or 0 if the term is not present in the query), but we do normalize the document vector weights by the magnitude of the document vector (square root of the sum of squares of the tf-idf weights). Finally, the cosine scores are the dot product of the query vector and the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e95deb2-a809-43d4-96ac-0d8d9684efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "        scores = [0.0 for xx in range(len(self.titles))]\n",
    "        \n",
    "        query_weights = {}\n",
    "        for term in query:\n",
    "            query_weights[term] = 1 + math.log10(query.count(term)) \n",
    "        \n",
    "        \n",
    "        doc_lengths = [0.0 for xx in range(len(self.titles))]\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "                length = 0.0\n",
    "                for word in set(doc):\n",
    "                    length += self.get_tfidf(word, doc_id) ** 2 #sums up the square of the tf-idf weight of each word in the doc\n",
    "                doc_lengths[doc_id] = math.sqrt(length) #square root of the sum in order to get the length of the vector |d|\n",
    "        \n",
    "        \n",
    "        for term in query_weights:\n",
    "            postings = self.get_posting(term) #find all the documents contain t in the query\n",
    "            for doc_id in postings:\n",
    "                scores[doc_id] += self.get_tfidf(term, doc_id) * query_weights[term] #numerator of the cosine similarity \n",
    "\n",
    "                \n",
    "        for doc_id in range(len(scores)):\n",
    "            if doc_lengths[doc_id] > 0:\n",
    "                scores[doc_id] /= doc_lengths[doc_id] #normalize the scores\n",
    "\n",
    "                \n",
    "        ranking = [idx for idx, sim in sorted(enumerate(scores), key=lambda xx: xx[1], reverse=True)]\n",
    "        results = []\n",
    "        for i in range(10): #return only the top 10 documents related to the query and their tfidf score\n",
    "            results.append((ranking[i], scores[ranking[i]]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ab645-8063-48e4-914a-f2d8016dfeff",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. \n",
    "\n",
    "You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbda6ab-a4f5-4237-87d2-92602fa0ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem): \n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931a01d-38c1-4126-b278-89895c2cbe59",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the `run_tests` and `run_query` functions to test your code. `run_tests` tests how different components your search engine code perform on a small set of queries and checks whether or not it matches up with the solution's results. `run_query` can be used to test your code on individual queries.\n",
    "\n",
    "Note that the first run for either of these functions might take a little while, since it will stem all the words in every document create a directory named `stemmed/` in `../data/ProjectGutenberg/`. This is meant to be a simple cache for the stemmed versions of the text documents. Later runs will be much faster after the first run since all the stemming will already be completed. However, this means that if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in `../data/ProjectGutenberg/stemmed/`. If this happens, simply remove the `stemmed/` directory and re-run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adce6831-0100-4403-9ffa-cbf94a90e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(5):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "        \n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and \\\n",
    "                        guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and \\\n",
    "                            top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % \\\n",
    "                   (num_correct, num_total, float(num_correct) / num_total)\n",
    "        print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924d64ec-ab03-4f25-b5db-004382e51b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "===== Running tests =====\n",
      "Inverted Index Test\n",
      "6/6 Correct. Accuracy: 1.000000\n",
      "Get Postings Test\n",
      "6/6 Correct. Accuracy: 1.000000\n",
      "Boolean Retrieval Test\n",
      "6/6 Correct. Accuracy: 1.000000\n",
      "TF-IDF Test\n",
      "6/6 Correct. Accuracy: 1.000000\n",
      "Cosine Similarity Test\n",
      "6/6 Correct. Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to run the tests\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/ProjectGutenberg')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "run_tests(irsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89833f7b-466a-4310-b30a-15b31f23a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'My very own query':\n",
      "Anna Karenina - Leo Tolstoy: 1.477895e-02\n",
      "The Scarlet Letter - Nathaniel Hawthorne: 1.324621e-02\n",
      "Around the World in Eighty Days - Jules Verne: 1.291871e-02\n",
      "Dracula - Bram Stoker: 1.134571e-02\n",
      "Violin Mastery_Talks with Master Violinists and Teachers - Frederick Herman Martens: 1.129498e-02\n",
      "The Iliad - Homer: 9.175461e-03\n",
      "War and Peace - Leo Tolstoy: 6.636483e-03\n",
      "Don Quixote - Miguel de Cervantes Saavedra: 6.257655e-03\n",
      "The Count of Monte Cristo - Alexandre Dumas and Auguste Maquet: 5.863665e-03\n",
      "Our Vanishing Wild Life_Its Extermination and Preservation - William T Hornaday: 5.861535e-03\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/ProjectGutenberg')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "    \n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "        \n",
    "# Run any query you want!\n",
    "run_query(\"My very own query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578a112-2bf2-4a2d-a642-fce18d6a9d01",
   "metadata": {},
   "source": [
    "For reference, you can run the cell below to see the titles of all the documents in our dataset. As sanity checks, you can try tailoring your queries in run_query to output certain titles and/or checking what IR system outputs against the list of titles to see if the results make sense (i.e. the book Great Pianists on Piano Playing should probably be among the top results if the query is \"pianists play piano\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa36a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'pianists play piano':\n",
      "Great Pianists on Piano Playing - James Francis Cooke: 9.924595e-02\n",
      "Life of Chopin - Franz Liszt: 6.284168e-02\n",
      "Violin Mastery_Talks with Master Violinists and Teachers - Frederick Herman Martens: 5.756910e-02\n",
      "Resonance in Singing and Speaking - Thomas Fillebrown: 3.268530e-02\n",
      "The Sorrows of Young Werther - Johann Wolfgang von Goethe: 2.782459e-02\n",
      "Concerning the Spiritual in Art - Wassily Kandinsky: 2.308783e-02\n",
      "The Phantom of the Opera - Gaston Leroux: 1.518331e-02\n",
      "Crime and Punishment - Fyodor Dostoyevsky: 1.092031e-02\n",
      "Around the World in Eighty Days - Jules Verne: 1.042157e-02\n",
      "Anna Karenina - Leo Tolstoy: 9.780570e-03\n"
     ]
    }
   ],
   "source": [
    "run_query(\"pianists play piano\") #the book 'Great Pianists on Piano Playing' should probably be among the top results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787ceda",
   "metadata": {},
   "source": [
    "![](http://www.quickmeme.com/img/5c/5c7dcf024101b083008e90529f42c1e32be6a97d47fc4c0c8f449466b9bc8613.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
